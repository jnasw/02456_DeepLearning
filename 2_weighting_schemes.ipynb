{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eba2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from src.nn.nn_dataset import DataSampler\n",
    "from src.ode.sm_models_d import SynchronousMachineModels\n",
    "from src.nn.nn_actions import NeuralNetworkActions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f091fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Device detection\n",
    "def detect_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device\n",
    "\n",
    "\n",
    "device = detect_device()\n",
    "\n",
    "# Load configuration and apply overrides for fast debugging\n",
    "cfg = OmegaConf.load(\"src/conf/setup_dataset_nn.yaml\")\n",
    "\n",
    "# Debug-friendly settings\n",
    "cfg.nn.type = \"DynamicNN\"\n",
    "cfg.nn.lr = 1e-3\n",
    "cfg.nn.num_epochs = 20\n",
    "cfg.nn.early_stopping = False\n",
    "\n",
    "# Reduce dataset size for quick tests\n",
    "cfg.dataset.perc_of_data_points = 0.05\n",
    "cfg.dataset.perc_of_col_points = 0.05\n",
    "\n",
    "# Weighting scheme being tested\n",
    "cfg.nn.weighting.update_weights_freq = 5\n",
    "cfg.nn.weighting.update_weight_method = \"ID\"   # or \"Static\", \"ID\", \"DN\", \"WB\", \"Sam\"\n",
    "\n",
    "# Dataset path\n",
    "dataset_path = \"data/SM_AVR_GOV/dataset_set5_mixed.pkl\"\n",
    "assert os.path.exists(dataset_path), f\"Missing dataset: {dataset_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b810fbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SM_AVR_GOV data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonaswiendl/local/02456_DeepLearning/src/nn/nn_dataset.py:186: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)\n",
      "  training_sample = torch.tensor(training_sample, dtype=torch.float32) # convert the trajectory to tensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  800000 Number of validation samples:  100000 Number of testing samples:  100000\n",
      "Number of different initial conditions for collocation points:  100\n",
      "['theta', 'omega', 'E_d_dash', 'E_q_dash', 'R_F', 'V_r', 'E_fd', 'P_sv', 'P_m'] Variables\n",
      "[[-2, 2], [-1, 1], [0], [1], [1], [1.105], [1.08], [0.7048], [0.7048]] Set of values for init conditions\n",
      "[10, 10, 1, 1, 1, 1, 1, 1, 1] Iterations per value\n",
      "Selected deep learning model:  DynamicNN\n",
      "Number of labeled training data: 40000 Number of collocation points: 5000 Number of collocation points (IC): 100 Number of validation data: 100000\n",
      "Weights initialized as:  [1, 0.001, 0.0001, 0.001]  are updated with scheme:  ID every 5 epochs\n",
      "getting in training\n",
      "Initializing smoothing\n",
      "[SMOOTH] data=3.3861e-01, dt=5.0704e+01, pinn=3.9077e+00, ic=1.8269e+00\n",
      "[SMOOTH] data=3.3700e-01, dt=5.0722e+01, pinn=3.9214e+00, ic=1.8408e+00\n",
      "[SMOOTH] data=3.3521e-01, dt=5.0742e+01, pinn=3.9322e+00, ic=1.8554e+00\n",
      "[SMOOTH] data=3.3340e-01, dt=5.0760e+01, pinn=3.9390e+00, ic=1.8700e+00\n",
      "\n",
      "================ ID UPDATE ==================\n",
      "Epoch: 4\n",
      "Mean |f_i| per state: [0.05320001393556595, 2.373758554458618, 0.11624138057231903, 0.23752623796463013, 0.3771734833717346, 4.4317545890808105, 3.595513343811035, 0.11278901249170303, 0.1862749457359314]\n",
      "Inverse dynamics weights (raw): [18.79698371887207, 0.4212728440761566, 8.602787971496582, 4.210061073303223, 2.6512999534606934, 0.22564426064491272, 0.2781243920326233, 8.866110801696777, 5.368408203125]\n",
      "Normalized inverse dynamics weights: [0.380346417427063, 0.008524219505488873, 0.17407259345054626, 0.08518822491168976, 0.053647566586732864, 0.004565785173326731, 0.005627691280096769, 0.17940078675746918, 0.10862673074007034]\n",
      "Expanded ID weight vector (len=20):\n",
      "[1.0, 0.380346417427063, 0.008524219505488873, 0.17407259345054626, 0.08518822491168976, 0.053647566586732864, 0.004565785173326731, 0.005627691280096769, 0.17940078675746918, 0.10862673074007034, 0.380346417427063, 0.008524219505488873, 0.17407259345054626, 0.08518822491168976, 0.053647566586732864, 0.004565785173326731, 0.005627691280096769, 0.17940078675746918, 0.10862673074007034, 0.0010000000474974513]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (20) must match the size of tensor b (4) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m net = NeuralNetworkActions(cfg, modelling_full, data_loader=ds)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Call pinn_train2 EXACTLY how it was designed\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpinn_train2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_of_skip_data_points\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_of_skip_col_points\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# correct for new code\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_of_skip_val_points\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwandb_run\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Print summary\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/local/02456_DeepLearning/src/nn/nn_actions.py:948\u001b[39m, in \u001b[36mNeuralNetworkActions.pinn_train2\u001b[39m\u001b[34m(self, num_of_skip_data_points, num_of_skip_col_points, num_of_skip_val_points, wandb_run)\u001b[39m\n\u001b[32m    946\u001b[39m     \u001b[38;5;28mself\u001b[39m.weighting_scheme.update_weights_MA(epoch)\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.nn.weighting.update_weight_method==\u001b[33m\"\u001b[39m\u001b[33mID\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweighting_scheme\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_weights_ID\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_last_ode1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# log some plots to wandb\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wandb_run \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/local/02456_DeepLearning/src/nn/gradient_based_weighting.py:380\u001b[39m, in \u001b[36mPINNWeighting.update_weights_ID\u001b[39m\u001b[34m(self, epoch, ode_values)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28mprint\u001b[39m(full_weights.tolist())\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     \u001b[38;5;28mself\u001b[39m.weights = \u001b[43mfull_weights\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight_mask\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinal ID weights assigned.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    383\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=============================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (20) must match the size of tensor b (4) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# Load data + model physics\n",
    "ds = DataSampler(cfg, dataset_path=dataset_path)\n",
    "modelling_full = SynchronousMachineModels(cfg)\n",
    "\n",
    "# Create NeuralNetworkActions instance\n",
    "net = NeuralNetworkActions(cfg, modelling_full, data_loader=ds)\n",
    "# Call pinn_train2 EXACTLY how it was designed\n",
    "net.pinn_train2(\n",
    "    num_of_skip_data_points=1,\n",
    "    num_of_skip_col_points=1,      # correct for new code\n",
    "    num_of_skip_val_points=1,\n",
    "    wandb_run=None,\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nTraining completed.\")\n",
    "print(f\"Final total loss: {net.loss_total.item():.4e}\")\n",
    "print(f\"Data loss:        {net.loss_data.item():.4e}\")\n",
    "print(f\"dt loss:          {net.loss_dt.item():.4e}\")\n",
    "print(f\"PINN loss:        {net.loss_pinn.item():.4e}\")\n",
    "print(f\"IC loss:          {net.loss_pinn_ic.item():.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c2d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
