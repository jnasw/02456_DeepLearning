wandb:
  api_key: "Add your wandb key here"
  project: "Project_PINNs_for_synchronous_machine"

time : 1 # Total time to be considered
num_of_points : 1000 # To be used when defining new collocation points, num per second, e.g. 1000*1s = 5K points
modelling_method : True # Modelling approach to be followed, ddelta = omega or ddelta = omega*Omega_B
seed : 37 # Seed for reproducibility

model:
  model_flag : "SM_AVR_GOV" # Choose between "SM_IB" or "SM4" or "SM_AVR" or "SM_AVR_GOV" or "SM6"
  machine_num : 7 # Set of machine parameters to be used
  init_condition_bounds : 1 # Final number of set of initial conditions for the collocation points to be used
  sampling: "Lhs" # Choose between "Lhs" or "Linear" or "Random" for sampling the initial conditions for collocation points
  torch : True # Keep it True when training the model! and also using to create the collocaiton points
  
dataset:
  number : 1 # Number of dataset to be used
  shuffle: True # Shuffle the trajectories of the dataset # If True, the dataset will be shuffled
  split_ratio: 0.8 # Train and validation/test split ratio, the rest is splitted equally for validation/test
  validation_flag : True # Use validation set
  transform_input : "None" # Choose between "Std" or "MinMax" or "None" for input transformation
  transform_output : "None" # Choose between "Std" or "MinMax" or "None" for output transformation
  new_coll_points_flag : True # Use new collocation points or use from the dataset
  perc_of_data_points : 1 # Percentage of ground truth data points to be used for PINN training
  perc_of_col_points : 1 # Percentage of available collaction points to be used for PINN training

nn: 
  type : "DynamicNN" # Choose between "StaticNN" or "DynamicNN" or "PinnA" or "PinnB" or "PinnAA" this is the adj target one 
  hidden_layers: 4
  hidden_dim: 64 # 20 if Kan
  loss_criterion : "MSELoss" # Choose between "MSELoss" or "L1Loss" or "SmoothL1Loss"
  time_factored_loss : False # Use time factored loss or not
  optimizer : "LBFGS" # Choose between "Adam" or "SGD" or "RMSprop" or "LBFGS"
  weight_init: "xavier_normal" # Choose between "xavier_normal" or "xavier_uniform" or "kaiming_normal" or "kaiming_uniform" or "normal"
  lr: 0.001
  lr_scheduler : "No_scheduler" # Choose between "StepLR", "MultiStepLR", "ExponentialLR", "ReduceLROnPlateau", "No_scheduler"
  num_epochs: 15000
  batch_size: "None" # Choose between "None" or any integer value
  early_stopping: True 
  early_stopping_patience: 2500
  early_stopping_min_delta: 1e-6
  weighting:
    flag_mean_weights : True # Create individual weights for each residual term of loss dt and loss pinn or not
    two_phase_weights : False # Use two phase static weights or not
    switch_epoch : 500 # Epoch to switch weights if two_phase_weights is True
    update_weight_method : "Static" # Choose between "Static", "ReLoBRaLo", "Dynamic", "Sam", "Gradient", "Ntk"
    Beta: 0.99
    bias_correction : True # Whether to use bias correction in Dynamic weight update
    update_weights_freq : 50 # if Dynamic or Sam, value must be >=1
    weights: [1, 1e-3, 1e-4, 1e-3] # [1, 1e-3, 1e-4, 1e-3] [0, 0, 1e-4, 1e-3]
  multi_optim: False
  switch_optim_epoch: 500 # Epoch to switch from Adam to LBFGS if multi_optim is True
  optimizer_2:
    lr: 0.0001
    betas: [0.9, 0.999]
  adaptive_col:
    enabled: false
    adapt_every: 50
    eval_grid:
      n_time: 50
      theta_range: [-2, 2]
      omega_range: [-1, 1]
    strategy: "augment"     # {"augment", "resample"}
    ratio: 0.3               # percent of high-residual points to add/replace
    save_snapshots: True # Save collocation points and residuals at each adaptation step

dirs:
  params_dir :  src/conf/params #${hydra:runtime.cwd}/src/conf/params
  init_conditions_dir : src/conf/initial_conditions #${hydra:runtime.cwd}/src/conf/initial_conditions
  dataset_dir : data #${hydra:runtime.cwd}/data
  model_dir : model #${hydra:runtime.cwd}/model